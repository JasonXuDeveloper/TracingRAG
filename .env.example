# TracingRAG Configuration

# Application
APP_NAME=TracingRAG
APP_VERSION=0.2.0
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# LLM Provider (OpenRouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# LLM Retry Configuration
# Retry mechanism for handling rate limits (429) and transient errors
LLM_MAX_RETRIES=5
LLM_RETRY_BASE_DELAY=1.0
LLM_RETRY_MAX_DELAY=60.0
FALLBACK_LLM_MAX_RETRIES=3

# Unified model strategy: GPT-4o-mini for all tasks
# Strategy: Use GPT-4o-mini for best reasoning accuracy and structured output reliability
#
# PRIMARY & FALLBACK: OpenAI GPT-4o-mini
#   - üß† GPT-4 level reasoning in compact form
#   - üí∞ Affordable: $0.15/1M input, $0.60/1M output
#   - ‚úÖ 128K context window
#   - ‚úÖ Excellent structured output (JSON schema) stability - no format errors
#   - ‚úÖ Strong reasoning for relationship analysis, conflict detection, synthesis
#   - ‚úÖ High accuracy and consistency
#   - üí° Cost estimate: ~$5-10/month for typical usage (100 memories/day)
#
# Why GPT-4o-mini over Gemini 2.5 Flash Lite:
#   - ‚úÖ Better reasoning ability (GPT-4 vs lightweight Flash)
#   - ‚úÖ More reliable structured output (Gemini Lite has known JSON bugs)
#   - ‚úÖ Higher accuracy for critical tasks (relationship/conflict analysis)
#   - ‚ö†Ô∏è  Only 1.5x more expensive (~$3-5/month difference)
#
# For TracingRAG's core tasks (relationship analysis, conflict detection, content synthesis),
# accuracy and reliability are critical - GPT-4o-mini is the best choice.

# All models use GPT-4o-mini
DEFAULT_LLM_MODEL=openai/gpt-4o-mini
FALLBACK_LLM_MODEL=openai/gpt-4o-mini
PLANNER_MODEL=openai/gpt-4o-mini
MANAGER_MODEL=openai/gpt-4o-mini
ANALYSIS_MODEL=openai/gpt-4o-mini
EVALUATION_MODEL=openai/gpt-4o-mini
QUERY_ANALYZER_MODEL=openai/gpt-4o-mini

# Relationship Management
# Use intelligent LLM-based relationship updates on evolution (recommended for production)
# If false, uses simple inheritance (faster but less accurate)
INTELLIGENT_RELATIONSHIP_UPDATES=true
# Minimum embedding similarity to consider (0.0 = no filter, get all)
# Set lower for comprehensive search, higher for precision
RELATIONSHIP_UPDATE_SIMILARITY_THRESHOLD=0.45
# Max candidates for LLM batch processing (will process all in batches)
RELATIONSHIP_UPDATE_LLM_BATCH_SIZE=30

# Cascading Evolution
# Single-level evolution of related topics when new memory is created
# Note: Only evolves directly related topics (no multi-level cascade)
ENABLE_CASCADING_EVOLUTION=true
# Minimum similarity for evolution candidates (higher = more selective)
CASCADING_EVOLUTION_SIMILARITY_THRESHOLD=0.45
# Process this many topics per LLM call (smaller = less token usage, more parallel calls)
CASCADING_EVOLUTION_BATCH_SIZE=5

# Embedding Configuration
# Local model (default) - runs on your machine, no API costs
# Recommended models:
#  - sentence-transformers/all-mpnet-base-v2 (768 dim, English, good quality)
#  - sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768 dim, 50+ languages)
#  - sentence-transformers/all-MiniLM-L6-v2 (384 dim, English, faster)
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DIMENSION=768

# Qdrant (Vector Database)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=tracingrag_memories
QDRANT_GRPC_PORT=6334
QDRANT_USE_GRPC=false

# Neo4j (Graph Database)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=tracingrag123
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# Neo4j Batch Processing
# Maximum number of relationships to create in a single transaction
# Lower values = safer (less memory/log usage), but more transactions
# Higher values = faster, but may cause transaction log errors with large batches
NEO4J_BATCH_SIZE=100
# Maximum number of memory states to process concurrently for relationship updates
# Lower values = less concurrent load on Neo4j, more stable
# Higher values = faster processing, but may overwhelm Neo4j
NEO4J_MAX_CONCURRENT_UPDATES=5

# PostgreSQL (Document Store)
DATABASE_URL=postgresql+asyncpg://tracingrag:tracingrag123@localhost:5432/tracingrag
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10
DATABASE_ECHO=false

# Redis (Caching)
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=10
CACHE_TTL=3600
CACHE_ENABLED=true

# Retrieval Configuration
DEFAULT_RETRIEVAL_LIMIT=10
MAX_RETRIEVAL_LIMIT=100
DEFAULT_GRAPH_DEPTH=2
MAX_GRAPH_DEPTH=5
ENABLE_HYBRID_SEARCH=true

# Memory Promotion
PROMOTION_CONFIDENCE_THRESHOLD=0.7
AUTO_PROMOTION_ENABLED=false
MAX_TRACE_HISTORY_CONTEXT=10

# Agent Configuration
AGENT_MAX_ITERATIONS=10
AGENT_TIMEOUT_SECONDS=900
ENABLE_AGENT_MEMORY=true
AGENT_THINKING_BUDGET=1000

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9100
ENABLE_TRACING=false
JAEGER_ENDPOINT=http://localhost:14268/api/traces

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# Security
SECRET_KEY=your-secret-key-here-change-in-production
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000
ALLOWED_HOSTS=*

# OpenAI (Optional - for automatic fallback if local model fails or better multilingual support)
# If OPENAI_API_KEY is set, will use OpenAI embeddings instead of local model
# OpenAI embeddings support 100+ languages with high quality
# Models: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
OPENAI_API_KEY=
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
