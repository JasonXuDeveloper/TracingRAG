# TracingRAG Configuration

# Application
APP_NAME=TracingRAG
APP_VERSION=0.2.0
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# LLM Provider (OpenRouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# LLM Retry Configuration
# Retry mechanism for handling rate limits (429) and transient errors
LLM_MAX_RETRIES=5
LLM_RETRY_BASE_DELAY=1.0
LLM_RETRY_MAX_DELAY=60.0
FALLBACK_LLM_MAX_RETRIES=3

# Unified model strategy: Qwen3-4B (free) for all tasks with intelligent fallback chain
# Strategy: Use Qwen3-4B:free as primary model with fallback to Qwen3-8B and GPT-4o-mini
#
# PRIMARY: Qwen/Qwen3-4B:free
#   - ðŸ’° Free tier model - no API costs!
#   - ðŸ§  Strong reasoning for a 4B parameter model
#   - âœ… Good structured output support
#   - âœ… 32K context window
#   - âš¡ Fast inference
#
# FALLBACK CHAIN:
#   1. qwen/qwen3-8b - More capable Qwen model with better reasoning
#   2. openai/gpt-4o-mini - Premium fallback with GPT-4 level reasoning
#      - $0.15/1M input, $0.60/1M output
#      - 128K context window
#      - Excellent structured output reliability
#
# Fallback strategy: Try free/cheaper models first, fall back to premium models only when needed.
# This maximizes cost efficiency while maintaining reliability.

# All models use Qwen3-4B:free as primary
DEFAULT_LLM_MODEL=qwen/qwen3-4b:free

# Fallback LLM models (comma-separated list, tried in order when primary model fails)
# Each model is retried FALLBACK_LLM_MAX_RETRIES times before moving to the next
FALLBACK_LLM_MODEL=qwen/qwen3-8b,openai/gpt-4o-mini

PLANNER_MODEL=qwen/qwen3-4b:free
MANAGER_MODEL=qwen/qwen3-4b:free
ANALYSIS_MODEL=qwen/qwen3-4b:free
EVALUATION_MODEL=qwen/qwen3-4b:free
QUERY_ANALYZER_MODEL=qwen/qwen3-4b:free

# Relationship Management
# Use intelligent LLM-based relationship updates on evolution (recommended for production)
# If false, uses simple inheritance (faster but less accurate)
INTELLIGENT_RELATIONSHIP_UPDATES=true
# Minimum embedding similarity to consider (0.0 = no filter, get all)
# Set lower for comprehensive search, higher for precision
RELATIONSHIP_UPDATE_SIMILARITY_THRESHOLD=0.45
# Max candidates for LLM batch processing (will process all in batches)
RELATIONSHIP_UPDATE_LLM_BATCH_SIZE=30

# Cascading Evolution
# Single-level evolution of related topics when new memory is created
# Note: Only evolves directly related topics (no multi-level cascade)
ENABLE_CASCADING_EVOLUTION=true
# Minimum similarity for evolution candidates (higher = more selective)
CASCADING_EVOLUTION_SIMILARITY_THRESHOLD=0.45
# Process this many topics per LLM call (smaller = less token usage, more parallel calls)
CASCADING_EVOLUTION_BATCH_SIZE=5

# Embedding Configuration
# Local model (default) - runs on your machine, no API costs
# Recommended models:
#  - sentence-transformers/all-mpnet-base-v2 (768 dim, English, good quality)
#  - sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768 dim, 50+ languages)
#  - sentence-transformers/all-MiniLM-L6-v2 (384 dim, English, faster)
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DIMENSION=768

# Qdrant (Vector Database)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=tracingrag_memories
QDRANT_GRPC_PORT=6334
QDRANT_USE_GRPC=false

# Neo4j (Graph Database)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=tracingrag123
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# Neo4j Batch Processing
# Maximum number of relationships to create in a single transaction
# Lower values = safer (less memory/log usage), but more transactions
# Higher values = faster, but may cause transaction log errors with large batches
NEO4J_BATCH_SIZE=100
# Maximum number of memory states to process concurrently for relationship updates
# Lower values = less concurrent load on Neo4j, more stable
# Higher values = faster processing, but may overwhelm Neo4j
NEO4J_MAX_CONCURRENT_UPDATES=5

# PostgreSQL (Document Store)
DATABASE_URL=postgresql+asyncpg://tracingrag:tracingrag123@localhost:5432/tracingrag
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10
DATABASE_ECHO=false

# Redis (Caching)
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=10
CACHE_TTL=3600
CACHE_ENABLED=true

# Retrieval Configuration
DEFAULT_RETRIEVAL_LIMIT=10
MAX_RETRIEVAL_LIMIT=100
DEFAULT_GRAPH_DEPTH=2
MAX_GRAPH_DEPTH=5
ENABLE_HYBRID_SEARCH=true

# Memory Promotion
PROMOTION_CONFIDENCE_THRESHOLD=0.7
AUTO_PROMOTION_ENABLED=false
MAX_TRACE_HISTORY_CONTEXT=10

# Agent Configuration
AGENT_MAX_ITERATIONS=10
AGENT_TIMEOUT_SECONDS=900
ENABLE_AGENT_MEMORY=true
AGENT_THINKING_BUDGET=1000

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9100
ENABLE_TRACING=false
JAEGER_ENDPOINT=http://localhost:14268/api/traces

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# Security
SECRET_KEY=your-secret-key-here-change-in-production
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000
ALLOWED_HOSTS=*

# OpenAI (Optional - for automatic fallback if local model fails or better multilingual support)
# If OPENAI_API_KEY is set, will use OpenAI embeddings instead of local model
# OpenAI embeddings support 100+ languages with high quality
# Models: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
OPENAI_API_KEY=
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
