# TracingRAG Configuration

# Application
APP_NAME=TracingRAG
APP_VERSION=0.2.0
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# LLM Provider (OpenRouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# LLM Retry Configuration
# Retry mechanism for handling rate limits (429) and transient errors
LLM_MAX_RETRIES=5
LLM_RETRY_BASE_DELAY=1.0
LLM_RETRY_MAX_DELAY=60.0
FALLBACK_LLM_MAX_RETRIES=3

# Unified model strategy: Google Gemini 2.5 Flash Lite (PAID BUT CHEAP)
# Strategy: Use stable paid Gemini for reliability - free models have unstable rate limits
#
# PRIMARY & FALLBACK: Google Gemini 2.5 Flash Lite
#   - ‚ö° Ultra-fast speed and high reliability
#   - üí∞ Very cheap: $0 input, $0.000003 output (~$0.003 per 1M tokens output)
#   - ‚úÖ 1M context window - handles massive context
#   - ‚úÖ Native response_format support - excellent JSON stability
#   - ‚úÖ No rate limit issues (unlike free models)
#   - ‚úÖ Consistent performance for partition + mapreduce
#   - üí° Cost estimate: ~$0.30 for 100M tokens output (negligible)
#
# Why not free models:
#   - ‚ùå Gemini Flash Experimental: Unstable, frequent 429 errors
#   - ‚ùå Mistral Small 3.2 Free: Also rate-limited upstream
#   - ‚ùå Other free models: Unreliable for production use

# All models use Gemini 2.5 Flash Lite (PAID BUT CHEAP)
DEFAULT_LLM_MODEL=google/gemini-2.5-flash-lite
FALLBACK_LLM_MODEL=google/gemini-2.5-flash-lite
PLANNER_MODEL=google/gemini-2.5-flash-lite
MANAGER_MODEL=google/gemini-2.5-flash-lite
ANALYSIS_MODEL=google/gemini-2.5-flash-lite
EVALUATION_MODEL=google/gemini-2.5-flash-lite
QUERY_ANALYZER_MODEL=google/gemini-2.5-flash-lite

# Relationship Management
# Use intelligent LLM-based relationship updates on evolution (recommended for production)
# If false, uses simple inheritance (faster but less accurate)
INTELLIGENT_RELATIONSHIP_UPDATES=true
# Minimum embedding similarity to consider (0.0 = no filter, get all)
# Set lower for comprehensive search, higher for precision
RELATIONSHIP_UPDATE_SIMILARITY_THRESHOLD=0.4
# Max candidates for LLM batch processing (will process all in batches)
RELATIONSHIP_UPDATE_LLM_BATCH_SIZE=30

# Embedding Configuration
# Local model (default) - runs on your machine, no API costs
# Recommended models:
#  - sentence-transformers/all-mpnet-base-v2 (768 dim, English, good quality)
#  - sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768 dim, 50+ languages)
#  - sentence-transformers/all-MiniLM-L6-v2 (384 dim, English, faster)
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DIMENSION=768

# Qdrant (Vector Database)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=tracingrag_memories
QDRANT_GRPC_PORT=6334
QDRANT_USE_GRPC=false

# Neo4j (Graph Database)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=tracingrag123
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# PostgreSQL (Document Store)
DATABASE_URL=postgresql+asyncpg://tracingrag:tracingrag123@localhost:5432/tracingrag
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10
DATABASE_ECHO=false

# Redis (Caching)
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=10
CACHE_TTL=3600
CACHE_ENABLED=true

# Retrieval Configuration
DEFAULT_RETRIEVAL_LIMIT=10
MAX_RETRIEVAL_LIMIT=100
DEFAULT_GRAPH_DEPTH=2
MAX_GRAPH_DEPTH=5
ENABLE_HYBRID_SEARCH=true

# Memory Promotion
PROMOTION_CONFIDENCE_THRESHOLD=0.7
AUTO_PROMOTION_ENABLED=false
MAX_TRACE_HISTORY_CONTEXT=10

# Agent Configuration
AGENT_MAX_ITERATIONS=10
AGENT_TIMEOUT_SECONDS=900
ENABLE_AGENT_MEMORY=true
AGENT_THINKING_BUDGET=1000

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9100
ENABLE_TRACING=false
JAEGER_ENDPOINT=http://localhost:14268/api/traces

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# Security
SECRET_KEY=your-secret-key-here-change-in-production
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000
ALLOWED_HOSTS=*

# OpenAI (Optional - for automatic fallback if local model fails or better multilingual support)
# If OPENAI_API_KEY is set, will use OpenAI embeddings instead of local model
# OpenAI embeddings support 100+ languages with high quality
# Models: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
OPENAI_API_KEY=
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
